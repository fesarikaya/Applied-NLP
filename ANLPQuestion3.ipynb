{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "259FEqEb5xRr"
   },
   "source": [
    "\n",
    "\n",
    "#Question 3 (50 Marks)\n",
    "\n",
    "This question is about Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "89Wx4WPa5weU"
   },
   "outputs": [],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "import nltk\n",
    "\n",
    "# This is a list of sentences with NER tags which we will use in this question.\n",
    "tagged_sents=[[\"tim_PER\", \"cook_PER\", \"is\", \"the\", \"current\", \"ceo\", \"of\", \"apple_ORG\", \"inc._ORG\", \",\", \"headquartered\", \"in\", \"cupertino_LOC\", \".\"],\n",
    "              [\"fiona_PER\", \"apple_PER\", \"was\", \"born\", \"in\", \"new_LOC\", \"york_LOC\", \"and\", \"her\", \"debut\", \"album\", \"was\", \"released\", \"by\", \"columbia_ORG\", \"records_ORG\", \".\"],\n",
    "              [\"the_LOC\", \"big_LOC\", \"apple_LOC\", \"is\", \"a\", \"nickname\", \"for\", \"new_LOC\", \"york_LOC\", \"city_LOC\", \".\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MUMcHxbBsBi"
   },
   "source": [
    "a) Follow the steps below to preprocess the NER tagged sentences, producing a list of lists of tokens and a list of lists of NER tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FDwzz0oCL8P"
   },
   "source": [
    "i) Use the `.split()` method to separate the tokens from the NER tags in the list `tagged_sents`, and strip off the tags to produce a list of lists of strings, called `tokens`, corresponding to just the tokens in each sentence.\n",
    "\n",
    "So, for example `[[\"Alice_PER\", \"runs\"], [\"Bob_PER\", \"walks\"]]` would produce `[[\"Alice\", \"runs\"], [\"Bob\", \"walks\"]]`.\n",
    "\n",
    "(8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9fY75wgToDMo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['tim', 'cook', 'is', 'the', 'current', 'ceo', 'of', 'apple', 'inc.', ',', 'headquartered', 'in', 'cupertino', '.']\n",
      "['fiona', 'apple', 'was', 'born', 'in', 'new', 'york', 'and', 'her', 'debut', 'album', 'was', 'released', 'by', 'columbia', 'records', '.']\n",
      "['the', 'big', 'apple', 'is', 'a', 'nickname', 'for', 'new', 'york', 'city', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the list to hold lists of tokens\n",
    "tokens = []\n",
    "\n",
    "# Iterate over each sentence in tagged_sents\n",
    "for sent in tagged_sents:\n",
    "    # Use a list comprehension to split each tagged token and take the first part (the token itself)\n",
    "    sent_tokens = [token.split('_')[0] for token in sent]\n",
    "    # Append the list of tokens for this sentence to the tokens list\n",
    "    tokens.append(sent_tokens)\n",
    "\n",
    "# Print the tokens to verify the output\n",
    "print(\"Tokens:\")\n",
    "for sent_tokens in tokens:\n",
    "    print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glog6pY1NkcV"
   },
   "source": [
    "ii) Use the `.split()` method to separate the tokens from the NER tags in the list `tagged_sents`, producing a list of lists of NER tags, called `tags`. Tokens without an NER tag should be tagged with the letter `\"O\"`.\n",
    "\n",
    "So, for example `[[\"Alice_PER\", \"runs\"], [\"chase\", \"Bob_PER\"]]` would produce `[[\"PER\", \"O\"], [\"O\", \"PER\"]]`.\n",
    "\n",
    "(6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2ZgLxYBConpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Tags:\n",
      "['PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'O', 'O', 'O', 'LOC', 'O']\n",
      "['PER', 'PER', 'O', 'O', 'O', 'LOC', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'O']\n",
      "['LOC', 'LOC', 'LOC', 'O', 'O', 'O', 'O', 'LOC', 'LOC', 'LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the list to hold lists of NER tags\n",
    "tags = []\n",
    "\n",
    "# Iterate over each sentence in tagged_sents\n",
    "for sent in tagged_sents:\n",
    "    # Use a list comprehension to split each tagged token and take the tag part, default to \"O\"\n",
    "    sent_tags = [token.split('_')[1] if '_' in token else 'O' for token in sent]\n",
    "    # Append the list of tags for this sentence to the tags list\n",
    "    tags.append(sent_tags)\n",
    "\n",
    "# Print the tags to verify the output\n",
    "print(\"NER Tags:\")\n",
    "for sent_tags in tags:\n",
    "    print(sent_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhzmSCQTRhJo"
   },
   "source": [
    "b) Now, follow the steps below to derive the transition and emission probabilities of a Hidden Markov Model for the NER tag sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lz2eIJ6Ry6w"
   },
   "source": [
    "i) Describe the two assumptions that a Hidden Markov Model for sequence tagging is based on.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2vWgcgIUnvp"
   },
   "source": [
    "A Hidden Markov Model (HMM) for sequence tagging is based on two key assumptions:\n",
    "<ol>\n",
    "    <li><b>Markov Assumption (or First-Order Markov Property)</b>: This assumption states that the probability of a particular state (or tag in the context of sequence tagging) depends only on the previous state. In other words, the future state is conditionally independent of the past states given the present state. For sequence tagging, this means that the probability of a tag at position $i$ in a sequence depends only on the tag at position $i-1$ and not on tags before $i-1$.</li>\n",
    "    <li><b>Output Independence (or Emission Independence)</b>: This assumption posits that the probability of an observed symbol (or token in the context of sequence tagging) being emitted by a state (or tag) is independent of the other observed symbols and the sequence of states that led up to the current state. In sequence tagging, this translates to the probability of a word being associated with a tag being independent of surrounding words and their respective tags. The observed word (emission) is only dependent on the current state (tag) that emits it.</li>\n",
    "</ol>\n",
    "\n",
    "These two assumptions allow HMMs to model the joint probability of a sequence of observed symbols (words in a sentence) and a sequence of states (tags) in a computationally tractable way. However, it's important to note that real-world sequences often have dependencies that go beyond these assumptions, which can limit the accuracy of HMMs. Despite this, HMMs can still perform quite well in many sequence tagging tasks like part-of-speech tagging, speech recognition, and, as in this case, named entity recognition (NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDlCGNMdSHBx"
   },
   "source": [
    "ii) Create a dictionary, called `nercounts`, containing the total counts for each tag in the list `tags`. The keys of `nercounts` should be tags and the values should be the frequencies of those tags.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ASbaMzO74wMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Counts:\n",
      "{'PER': 4, 'O': 25, 'ORG': 4, 'LOC': 9}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize defaultdict to hold tag counts (default to 0 for any key that does not exist)\n",
    "nercounts = defaultdict(int)\n",
    "\n",
    "# Iterate over each list of tags in the tags list\n",
    "for sent_tags in tags:\n",
    "    # Count the occurrences of each tag\n",
    "    for tag in sent_tags:\n",
    "        nercounts[tag] += 1\n",
    "\n",
    "# Convert defaultdict to a regular dictionary for the final output\n",
    "nercounts = dict(nercounts)\n",
    "\n",
    "# Print the nercounts to verify the output\n",
    "print(\"NER Counts:\")\n",
    "print(nercounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ54_g706lpv"
   },
   "source": [
    "iii) Using the lists `tags` and `tokens`, calculate the emission probabilities: $$p(token|tag)$$\n",
    "\n",
    "Put these probabilities in a dictionary of dictionaries, called `emission`, with outer keys being NER tags, inner keys being tokens and inner values being the emission probabilities.\n",
    "\n",
    "(8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ttpe2z36R2n0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission Probabilities:\n",
      "PER: {'tim': 0.25, 'cook': 0.25, 'fiona': 0.25, 'apple': 0.25}\n",
      "O: {'is': 0.08, 'the': 0.04, 'current': 0.04, 'ceo': 0.04, 'of': 0.04, ',': 0.04, 'headquartered': 0.04, 'in': 0.08, '.': 0.12, 'was': 0.08, 'born': 0.04, 'and': 0.04, 'her': 0.04, 'debut': 0.04, 'album': 0.04, 'released': 0.04, 'by': 0.04, 'a': 0.04, 'nickname': 0.04, 'for': 0.04}\n",
      "ORG: {'apple': 0.25, 'inc.': 0.25, 'columbia': 0.25, 'records': 0.25}\n",
      "LOC: {'cupertino': 0.1111111111111111, 'new': 0.2222222222222222, 'york': 0.2222222222222222, 'the': 0.1111111111111111, 'big': 0.1111111111111111, 'apple': 0.1111111111111111, 'city': 0.1111111111111111}\n"
     ]
    }
   ],
   "source": [
    "# Initialize nested defaultdict to hold emission counts\n",
    "# The outer dict holds tags, and the inner dict holds tokens and their counts\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Iterate over each sentence's tags and corresponding tokens\n",
    "for sent_tags, sent_tokens in zip(tags, tokens):\n",
    "    for tag, token in zip(sent_tags, sent_tokens):\n",
    "        # Count the occurrences of the token for the given tag\n",
    "        emission_counts[tag][token] += 1\n",
    "\n",
    "# Initialize the emission probability dictionary\n",
    "emission = defaultdict(dict)\n",
    "\n",
    "# Calculate the emission probabilities\n",
    "for tag, token_counts in emission_counts.items():\n",
    "    # Calculate the total count of the tag\n",
    "    total_tag_count = nercounts[tag]\n",
    "    # Calculate the emission probability for each token under the tag\n",
    "    for token, count in token_counts.items():\n",
    "        emission[tag][token] = count / total_tag_count\n",
    "\n",
    "# Convert defaultdict to a regular dictionary for the final output\n",
    "emission = {tag: dict(token_probs) for tag, token_probs in emission.items()}\n",
    "\n",
    "# Print the emission probabilities to verify the output\n",
    "print(\"Emission Probabilities:\")\n",
    "for tag, token_probs in emission.items():\n",
    "    print(f\"{tag}: {token_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugDBChyQ8MHu"
   },
   "source": [
    "iv) Using the list `tags`, calculate the transition probabilities: $$p(tag_i|tag_{i-1})$$\n",
    "\n",
    "You will need to keep track of both the current tag, $tag_i$, and the previous tag, $tag_{i-1}$, and you will need to introduce a special tag, e.g. \"START\", for the previous tag at the beginning of a sequence.\n",
    "\n",
    "For example, the sequence of tags `[\"PER\", \"O\"]` would give rise to the following values of $i$, $tag_{i-1}$ and $tag_i$:\n",
    "\n",
    "|i|$tag_{i-1}$|$tag_i$|\n",
    "|---|---|---|\n",
    "|0|START|PER|\n",
    "|1|PER|O|\n",
    "\n",
    "Calculate the conditional probabilities of $tag_i$ given $tag_{i-1}$ and put the results in a dictionary of dictionaries, called `transition`, with outer keys being previous tags, inner keys being current tags and inner values being transition probabilities.\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bbRg_hyU_76c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probabilities:\n",
      "START: {'PER': 0.6666666666666666, 'LOC': 0.3333333333333333}\n",
      "PER: {'PER': 0.5, 'O': 0.5}\n",
      "O: {'O': 0.7727272727272727, 'ORG': 0.09090909090909091, 'LOC': 0.13636363636363635}\n",
      "ORG: {'ORG': 0.5, 'O': 0.5}\n",
      "LOC: {'O': 0.4444444444444444, 'LOC': 0.5555555555555556}\n"
     ]
    }
   ],
   "source": [
    "# Initialize a defaultdict for transition counts\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Initialize counts for the special \"START\" tag\n",
    "start_tag_counts = defaultdict(int)\n",
    "\n",
    "# Iterate over each list of tags in the tags list\n",
    "for sent_tags in tags:\n",
    "    # Insert \"START\" at the beginning of each tag sequence\n",
    "    sent_tags = [\"START\"] + sent_tags\n",
    "    \n",
    "    # Count the transitions from each tag to the next tag\n",
    "    for i in range(len(sent_tags) - 1):\n",
    "        prev_tag = sent_tags[i]\n",
    "        curr_tag = sent_tags[i+1]\n",
    "        transition_counts[prev_tag][curr_tag] += 1\n",
    "        \n",
    "        # Increment the start tag count each time a sentence begins\n",
    "        if prev_tag == \"START\":\n",
    "            start_tag_counts[prev_tag] += 1\n",
    "\n",
    "# Initialize the transition probability dictionary\n",
    "transition = defaultdict(dict)\n",
    "\n",
    "# Calculate the transition probabilities\n",
    "for prev_tag, curr_tag_counts in transition_counts.items():\n",
    "    # Calculate the total count of the previous tag\n",
    "    total_prev_tag_count = sum(curr_tag_counts.values())\n",
    "    if prev_tag == \"START\":\n",
    "        total_prev_tag_count = start_tag_counts[prev_tag]\n",
    "    \n",
    "    # Calculate the transition probability for each current tag given the previous tag\n",
    "    for curr_tag, count in curr_tag_counts.items():\n",
    "        transition[prev_tag][curr_tag] = count / total_prev_tag_count\n",
    "\n",
    "# Convert defaultdict to a regular dictionary for the final output\n",
    "transition = {prev_tag: dict(curr_tag_probs) for prev_tag, curr_tag_probs in transition.items()}\n",
    "\n",
    "# Print the transition probabilities to verify the output\n",
    "print(\"Transition Probabilities:\")\n",
    "for prev_tag, curr_tag_probs in transition.items():\n",
    "    print(f\"{prev_tag}: {curr_tag_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDevraTjRH5X"
   },
   "source": [
    "v) Given a sequence of tokens, we want to find the most probable sequence of tags corresponding to those tokens. The brute force approach would simply evaluate the probability of every sequence of tags. Explain why we would want to avoid that approach and describe an alternative algorithm.\n",
    "\n",
    "(6 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7YwQI7MZxEY"
   },
   "source": [
    "The brute force approach to finding the most probable sequence of tags for a given sequence of tokens involves evaluating the probability of every possible sequence of tags, which is computationally infeasible for several reasons:\n",
    "<ol>\n",
    "    <li><b>Combinatorial Explosion</b>: For a sequence of tokens of length $n$` and a set of possible tags of size $t$, there are $t^n$ possible sequences of tags. For even moderately sized $n$ and $t$, this number becomes astronomically large, leading to a combinatorial explosion of possibilities.</li>\n",
    "    <li><b>Inefficiency</b>: Evaluating every possible sequence is highly inefficient because the vast majority of tag sequences are extremely unlikely and do not need to be considered. Brute force does not leverage the structure of the problem or any heuristics that could drastically reduce the search space.\n",
    "        <li><b>Time Complexity</b>: The time complexity of evaluating all possible sequences is exponential, making it impractical for real-time or even offline processing of sentences, especially when dealing with large corpora.</li>\n",
    "</ol>\n",
    "\n",
    "An alternative algorithm to the brute force approach is the <b>Viterbi algorithm</b>, which efficiently finds the most probable sequence of tags using dynamic programming. The Viterbi algorithm works as follows:\n",
    "<ol>\n",
    "    <li><b>Initialization</b>: Start with the probabilities of the first token being generated by each tag and multiply by the probability of each tag being at the start of a sentence (the \"<b>START</b>\" transition probabilities).</li>\n",
    "    <li><b>Recursion</b>: For each subsequent token, calculate the probabilities of each tag by considering the transition probabilities from all previous tags to the current tag and the emission probability of the current token given the current tag. Keep track of the most probable path to each tag.</li>\n",
    "    <li><b>Termination</b>: After processing all tokens, find the end state with the highest probability.</li>\n",
    "    <li><b>Path Backtracking</b>: Trace back through the saved paths to reconstruct the most probable sequence of tags from the end state to the start state.</li>\n",
    "\n",
    "The Viterbi algorithm exploits the Markov property of the HMM, which states that the probability of a tag at a given position depends only on the previous tag and not on the entire sequence of previous tags. By storing only the highest probabilities and paths at each step, the Viterbi algorithm avoids redundant calculations and reduces the problem to polynomial time complexity, specifically $O(n*t^2)$, which is much more manageable than the exponential complexity of the brute force approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdxdMimsLwpx"
   },
   "source": [
    "vi) If we are tagging a sequence and the current token is `\"apple\"` and the previous tag is `\"LOC\"`, then we want know to which tag in the current position  would maximise the probability of seeing that token.\n",
    "\n",
    "Define a function that takes a token, a previous tag, a dictionary of emission probabilities and a dictionary of transition probabilities and returns a dictionary of the probabilities $p(token, tag_i | tag_{i-1})$ for all values of $tag_i$.\n",
    "\n",
    "Apply this function to the case where the current token is `\"apple\"` and the previous tag is `\"LOC\"`.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8Me8nvZbRIbw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for the token 'apple' given the previous tag 'LOC':\n",
      "{'PER': 0.0, 'O': 0.0, 'ORG': 0.0, 'LOC': 0.06172839506172839}\n"
     ]
    }
   ],
   "source": [
    "def max_probability_token(token, prev_tag, emission_probs, transition_probs):\n",
    "    # Initialize a dictionary to hold the probabilities for the current token and all possible current tags\n",
    "    probabilities = {}\n",
    "    \n",
    "    # Iterate over all possible current tags\n",
    "    for curr_tag in emission_probs.keys():\n",
    "        # Calculate the transition probability for the current tag given the previous tag\n",
    "        trans_prob = transition_probs[prev_tag].get(curr_tag, 0)\n",
    "        \n",
    "        # Calculate the emission probability for the current token given the current tag\n",
    "        emit_prob = emission_probs[curr_tag].get(token, 0)\n",
    "        \n",
    "        # Calculate the combined probability of the current token and current tag given the previous tag\n",
    "        probabilities[curr_tag] = trans_prob * emit_prob\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "# Apply the function to the token \"apple\" and the previous tag \"LOC\"\n",
    "current_token = \"apple\"\n",
    "previous_tag = \"LOC\"\n",
    "probabilities = max_probability_token(current_token, previous_tag, emission, transition)\n",
    "\n",
    "# Print the probabilities to verify the output\n",
    "print(f\"Probabilities for the token '{current_token}' given the previous tag '{previous_tag}':\")\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
