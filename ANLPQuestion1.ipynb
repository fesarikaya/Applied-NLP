{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "259FEqEb5xRr"
   },
   "source": [
    "#Question 1 (50 Marks)\n",
    "\n",
    "This question is about document similarity and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "89Wx4WPa5weU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ferhatsarikaya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ferhatsarikaya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from math import log2, sqrt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "# This is corpus of quotations, their sources and languages which we will use in this question.\n",
    "corpus = [(\"All the world’s a stage, and all the men and women merely players.\", \"William Shakespeare\", \"English\"),\n",
    "          (\"Ask not what your country can do for you; ask what you can do for your country.\", \"John Kennedy\", \"English\"),\n",
    "          (\"Ask, and it shall be given you; seek, and you shall find.\", \"the Bible\", \"Greek\"),\n",
    "          (\"Eighty percent of success is showing up.\", \"Woody Allen\", \"English\"),\n",
    "          (\"Elementary, my dear Watson.\", \"Sherlock Holmes (character)\", \"English\"),\n",
    "          (\"For those to whom much is given, much is required.\", \"the Bible\", \"Greek\"),\n",
    "          (\"Frankly, my dear, I don't give a damn.\", \"Rhett Butler (character)\", \"English\"),\n",
    "          (\"Genius is one percent inspiration and ninety-nine percent perspiration.\", \"Thomas Edison\", \"English\")]\n",
    "\n",
    "# This is a query that we will retrieve relevant documents for.\n",
    "query = \"Will I be given what I ask?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee37cbR8ebN9"
   },
   "source": [
    "a) Preprocess the information in the list `corpus` by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGruck7ketTb"
   },
   "source": [
    "i) Create three dictionaries - `quotes`, `sources` and `languages` - corresponding to the three elements in each tuple of `corpus`. Each key in these dictionaries should be the position in the original list and each value should be a string from the tuple at that position in the list.\n",
    "\n",
    "Each value in `quotes` should be item 0 in a tuple, i.e. the quotation itself. Each value in `sources` should be item 1 in a tuple, i.e. the source associated with the quotation. Each value in `languages` should be item 2 in a tuple, i.e. the original language of the quotation.\n",
    "\n",
    "So for example, the corpus `[(\"Hello\", \"Alice\", \"English\")]` should be broken into three dictionaries: `{0: \"Hello\"}`, `{0: \"Alice\"}` and `{0: \"English\"}`.\n",
    "\n",
    "(7 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lf-jmySgFZAP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes Dictionary:\n",
      "{0: 'All the world’s a stage, and all the men and women merely players.', 1: 'Ask not what your country can do for you; ask what you can do for your country.', 2: 'Ask, and it shall be given you; seek, and you shall find.', 3: 'Eighty percent of success is showing up.', 4: 'Elementary, my dear Watson.', 5: 'For those to whom much is given, much is required.', 6: \"Frankly, my dear, I don't give a damn.\", 7: 'Genius is one percent inspiration and ninety-nine percent perspiration.'}\n",
      "\n",
      "Sources Dictionary:\n",
      "{0: 'William Shakespeare', 1: 'John Kennedy', 2: 'the Bible', 3: 'Woody Allen', 4: 'Sherlock Holmes (character)', 5: 'the Bible', 6: 'Rhett Butler (character)', 7: 'Thomas Edison'}\n",
      "\n",
      "Languages Dictionary:\n",
      "{0: 'English', 1: 'English', 2: 'Greek', 3: 'English', 4: 'English', 5: 'Greek', 6: 'English', 7: 'English'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionaries\n",
    "quotes, sources, languages = {}, {}, {}\n",
    "\n",
    "# Iterate through the corpus and populate the dictionaries\n",
    "for i, (quote, source, language) in enumerate(corpus):\n",
    "    quotes[i] = quote\n",
    "    sources[i] = source\n",
    "    languages[i] = language\n",
    "\n",
    "# Print the dictionaries to verify the output\n",
    "print(\"Quotes Dictionary:\")\n",
    "print(quotes)\n",
    "print(\"\\nSources Dictionary:\")\n",
    "print(sources)\n",
    "print(\"\\nLanguages Dictionary:\")\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HKbt1Npi6U_"
   },
   "source": [
    "ii) Tokenise the quotation strings in the dictionary `quotes` to produce a new dictionary with the same keys called `tokenised`, in which each value is a list of tokens.\n",
    "\n",
    "So, for example the dictionary `{0: \"Hello Bob\"}` would become `{0: [\"Hello\", \"Bob\"]}`.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VpJk-SrsGXSn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised Dictionary:\n",
      "{0: ['All', 'the', 'world', '’', 's', 'a', 'stage', ',', 'and', 'all', 'the', 'men', 'and', 'women', 'merely', 'players', '.'], 1: ['Ask', 'not', 'what', 'your', 'country', 'can', 'do', 'for', 'you', ';', 'ask', 'what', 'you', 'can', 'do', 'for', 'your', 'country', '.'], 2: ['Ask', ',', 'and', 'it', 'shall', 'be', 'given', 'you', ';', 'seek', ',', 'and', 'you', 'shall', 'find', '.'], 3: ['Eighty', 'percent', 'of', 'success', 'is', 'showing', 'up', '.'], 4: ['Elementary', ',', 'my', 'dear', 'Watson', '.'], 5: ['For', 'those', 'to', 'whom', 'much', 'is', 'given', ',', 'much', 'is', 'required', '.'], 6: ['Frankly', ',', 'my', 'dear', ',', 'I', 'do', \"n't\", 'give', 'a', 'damn', '.'], 7: ['Genius', 'is', 'one', 'percent', 'inspiration', 'and', 'ninety-nine', 'percent', 'perspiration', '.']}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenised dictionary\n",
    "tokenised = {}\n",
    "\n",
    "# Iterate through the quotes dictionary and tokenize each quote\n",
    "for key, quote in quotes.items():\n",
    "    tokenised[key] = word_tokenize(quote)\n",
    "\n",
    "# Print the tokenised dictionary to verify the output\n",
    "print(\"Tokenised Dictionary:\")\n",
    "print(tokenised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMa4g0kGjOze"
   },
   "source": [
    "iii) Case normalise the tokenised strings and remove stopwords and punctuation, putting the results into a new dictionary called `normalised`, with the same keys as `tokenised` and the values being normalised lists of tokens.\n",
    "\n",
    "(6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HZVk2RcMGQNY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised Dictionary:\n",
      "{0: ['world', '’', 'stage', 'men', 'women', 'merely', 'players'], 1: ['ask', 'country', 'ask', 'country'], 2: ['ask', 'shall', 'given', 'seek', 'shall', 'find'], 3: ['eighty', 'percent', 'success', 'showing'], 4: ['elementary', 'dear', 'watson'], 5: ['much', 'given', 'much', 'required'], 6: ['frankly', 'dear', \"n't\", 'give', 'damn'], 7: ['genius', 'one', 'percent', 'inspiration', 'ninety-nine', 'percent', 'perspiration']}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Initialize the normalised dictionary\n",
    "normalised = {}\n",
    "\n",
    "# Create a set of punctuation characters for easy removal\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "# Iterate through the tokenised dictionary and normalise each list of tokens\n",
    "for key, tokens in tokenised.items():\n",
    "    # Case normalize (to lowercase) and remove stopwords and punctuation\n",
    "    normalised[key] = [token.lower() for token in tokens if token.lower() not in stop and token not in punctuations]\n",
    "\n",
    "# Print the normalised dictionary to verify the output\n",
    "print(\"Normalised Dictionary:\")\n",
    "print(normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFcNSwE-mmdt"
   },
   "source": [
    "iv) Describe two other forms of pre-processing that could be applied to text documents.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9iw4TowmvAQ"
   },
   "source": [
    "Two other forms of pre-processing that could be applied to text documents in <b>NLP</b> are:\n",
    "\n",
    "<ol>\n",
    "    <li><b>Stemming</b>: Stemming is a process of reducing words to their word stem, base, or root form. For example, words like \"running\", \"runner\", and \"ran\" would be reduced to the root word \"run\". This helps in generalizing different forms of the same word so that they can be analyzed as a single item. It's a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time.</li>\n",
    "    <li><b>Lemmatization</b>: Lemmatization is similar to stemming but it brings context to the words. It looks beyond word reduction and considers a language's full vocabulary to apply a morphological analysis to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Lemmatization is typically seen as much more informative than stemming, which is why it's usually preferred if you have the computational resources for the more complex process.\n",
    "</ol>\n",
    "\n",
    "Both stemming and lemmatization are ways of processing words that allow us to consider word derivatives as a single word, which simplifies the representation of text and can improve the performance of text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzmBCqijjpj0"
   },
   "source": [
    "b) Convert each document, ie each list of tokens, into a tfidf representation by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c49faFMRdG0W"
   },
   "source": [
    "i) Calculate document frequencies for each token found in the documents contained in `normalised` and put the results in a dictionary called `df`. That is count how many different entries in the dictionary each token is found in.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wo96NEz6Hcz8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Frequency (DF) Dictionary:\n",
      "{'men': 1, 'women': 1, 'players': 1, 'stage': 1, 'merely': 1, 'world': 1, '’': 1, 'ask': 2, 'country': 1, 'given': 2, 'seek': 1, 'shall': 1, 'find': 1, 'eighty': 1, 'success': 1, 'percent': 2, 'showing': 1, 'elementary': 1, 'dear': 2, 'watson': 1, 'much': 1, 'required': 1, \"n't\": 1, 'give': 1, 'damn': 1, 'frankly': 1, 'inspiration': 1, 'perspiration': 1, 'genius': 1, 'ninety-nine': 1, 'one': 1}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the document frequency dictionary\n",
    "df = {}\n",
    "\n",
    "# Iterate through the normalised dictionary to calculate document frequencies\n",
    "for tokens in normalised.values():\n",
    "    # Use a set to avoid counting a token multiple times in a single document\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        # Increment the document frequency of the token\n",
    "        df[token] = df.get(token, 0) + 1\n",
    "\n",
    "# Print the document frequency dictionary to verify the output\n",
    "print(\"Document Frequency (DF) Dictionary:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67kNG26edztz"
   },
   "source": [
    "ii) Calculate inverse document frequencies from the document frequencies derived in the previous question and put the results in a dictionary called `idf`.\n",
    "\n",
    "$$IDF(w) = \\log_2 \\left( \\frac{N}{DF(w)} \\right) $$\n",
    "\n",
    "where $N$ is the total number of documents and $DF(w)$ is the document frequency of the word $w$.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wqTvv8c_wK0T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Document Frequency (IDF) Dictionary:\n",
      "{'men': 3.0, 'women': 3.0, 'players': 3.0, 'stage': 3.0, 'merely': 3.0, 'world': 3.0, '’': 3.0, 'ask': 2.0, 'country': 3.0, 'given': 2.0, 'seek': 3.0, 'shall': 3.0, 'find': 3.0, 'eighty': 3.0, 'success': 3.0, 'percent': 2.0, 'showing': 3.0, 'elementary': 3.0, 'dear': 2.0, 'watson': 3.0, 'much': 3.0, 'required': 3.0, \"n't\": 3.0, 'give': 3.0, 'damn': 3.0, 'frankly': 3.0, 'inspiration': 3.0, 'perspiration': 3.0, 'genius': 3.0, 'ninety-nine': 3.0, 'one': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the inverse document frequency dictionary\n",
    "idf = {}\n",
    "\n",
    "# Calculate N, the total number of documents\n",
    "N = len(normalised)\n",
    "\n",
    "# Calculate inverse document frequencies using the df dictionary\n",
    "for token, doc_freq in df.items():\n",
    "    # Apply the IDF formula\n",
    "    idf[token] = log2(N / doc_freq)\n",
    "\n",
    "# Print the IDF dictionary to verify the output\n",
    "print(\"Inverse Document Frequency (IDF) Dictionary:\")\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbTHONWreDTN"
   },
   "source": [
    "iii) Convert each document in `normalised` from a list of tokens to a dictionary of term frequencies and put the results in a dictionary called `tf`.\n",
    "\n",
    "The keys of `tf` should be positions in the original list `corpus` and the values should be the term frequency dictionaries, which map from tokens to frequency in each document.\n",
    "\n",
    "So, for example, `{0: [\"Hello\", \"Bob\"]}` would become `{0: {\"Hello\": 1, \"Bob\": 1}}`.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M5cfBQOTw7i_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency (TF) Dictionary:\n",
      "{0: {'world': 0.14285714285714285, '’': 0.14285714285714285, 'stage': 0.14285714285714285, 'men': 0.14285714285714285, 'women': 0.14285714285714285, 'merely': 0.14285714285714285, 'players': 0.14285714285714285}, 1: {'ask': 0.5, 'country': 0.5}, 2: {'ask': 0.16666666666666666, 'shall': 0.3333333333333333, 'given': 0.16666666666666666, 'seek': 0.16666666666666666, 'find': 0.16666666666666666}, 3: {'eighty': 0.25, 'percent': 0.25, 'success': 0.25, 'showing': 0.25}, 4: {'elementary': 0.3333333333333333, 'dear': 0.3333333333333333, 'watson': 0.3333333333333333}, 5: {'much': 0.5, 'given': 0.25, 'required': 0.25}, 6: {'frankly': 0.2, 'dear': 0.2, \"n't\": 0.2, 'give': 0.2, 'damn': 0.2}, 7: {'genius': 0.14285714285714285, 'one': 0.14285714285714285, 'percent': 0.2857142857142857, 'inspiration': 0.14285714285714285, 'ninety-nine': 0.14285714285714285, 'perspiration': 0.14285714285714285}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the term frequency dictionary\n",
    "tf = {}\n",
    "\n",
    "# Iterate through the normalised dictionary to calculate term frequencies\n",
    "for key, tokens in normalised.items():\n",
    "    # Initialize a dictionary for the current document's term frequencies\n",
    "    term_freqs = {}\n",
    "    # Calculate the total number of tokens in the current document\n",
    "    total_tokens = len(tokens)\n",
    "    for token in tokens:\n",
    "        # Increment the term frequency of the token\n",
    "        term_freqs[token] = term_freqs.get(token, 0) + 1\n",
    "    # Normalize the term frequencies by the total number of tokens in the document\n",
    "    for token in term_freqs.keys():\n",
    "        term_freqs[token] = term_freqs[token] / total_tokens\n",
    "    # Assign the term frequency dictionary to the corresponding document key in tf\n",
    "    tf[key] = term_freqs\n",
    "\n",
    "# Print the term frequency dictionary to verify the output\n",
    "print(\"Term Frequency (TF) Dictionary:\")\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSvH-9m9fWdV"
   },
   "source": [
    "iv) Convert the raw term frequencies in `tf` to tfidf values using the dictionary `idf`.\n",
    "\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WPL9avGW5rOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Dictionary:\n",
      "{0: {'world': 0.42857142857142855, '’': 0.42857142857142855, 'stage': 0.42857142857142855, 'men': 0.42857142857142855, 'women': 0.42857142857142855, 'merely': 0.42857142857142855, 'players': 0.42857142857142855}, 1: {'ask': 1.0, 'country': 1.5}, 2: {'ask': 0.3333333333333333, 'shall': 1.0, 'given': 0.3333333333333333, 'seek': 0.5, 'find': 0.5}, 3: {'eighty': 0.75, 'percent': 0.5, 'success': 0.75, 'showing': 0.75}, 4: {'elementary': 1.0, 'dear': 0.6666666666666666, 'watson': 1.0}, 5: {'much': 1.5, 'given': 0.5, 'required': 0.75}, 6: {'frankly': 0.6000000000000001, 'dear': 0.4, \"n't\": 0.6000000000000001, 'give': 0.6000000000000001, 'damn': 0.6000000000000001}, 7: {'genius': 0.42857142857142855, 'one': 0.42857142857142855, 'percent': 0.5714285714285714, 'inspiration': 0.42857142857142855, 'ninety-nine': 0.42857142857142855, 'perspiration': 0.42857142857142855}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tfidf dictionary\n",
    "tfidf = {}\n",
    "\n",
    "# Iterate through the term frequency dictionary to calculate tfidf values\n",
    "for key, term_freqs in tf.items():\n",
    "    # Initialize a dictionary for the current document's tfidf values\n",
    "    tfidf_values = {}\n",
    "    for token, tf_value in term_freqs.items():\n",
    "        # Calculate the tfidf value using the tf value and the corresponding idf value\n",
    "        tfidf_values[token] = tf_value * idf[token]\n",
    "    # Assign the tfidf dictionary to the corresponding document key in tfidf\n",
    "    tfidf[key] = tfidf_values\n",
    "\n",
    "# Print the tfidf dictionary to verify the output\n",
    "print(\"TFIDF Dictionary:\")\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKSzAq1zfj-L"
   },
   "source": [
    "c) In the following steps, preprocess the string `query` and convert it to a tfidf representation then use this to find relevant quotations in the index `tfidf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GF8dFWsHWlr"
   },
   "source": [
    "i) Define a function `dot` which takes two dictionaries containing tfidf values as inputs and calculates their dot product.\n",
    "\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ChDKHmrJFAiz"
   },
   "outputs": [],
   "source": [
    "def dot(dict1, dict2):\n",
    "    # Initialize the dot product result\n",
    "    dot_product = 0.0\n",
    "    # Iterate over the keys in the first dictionary\n",
    "    for key, value in dict1.items():\n",
    "        # If the key is also in the second dictionary, multiply the values and add to the dot product\n",
    "        if key in dict2:\n",
    "            dot_product += value * dict2[key]\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS6SEhUEHpsm"
   },
   "source": [
    "ii) Define a function, `sim`, which takes two dictionaries containing tfidf values as inputs and , using your `dot` function, calculates their cosine similarity.\n",
    "\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9kL3ufDkFOuX"
   },
   "outputs": [],
   "source": [
    "def sim(dict1, dict2):\n",
    "    # Calculate the dot product using the defined dot function\n",
    "    dot_product = dot(dict1, dict2)\n",
    "    \n",
    "    # Calculate the magnitude of the first dictionary\n",
    "    magnitude_dict1 = sqrt(sum([value**2 for value in dict1.values()]))\n",
    "    # Calculate the magnitude of the second dictionary\n",
    "    magnitude_dict2 = sqrt(sum([value**2 for value in dict2.values()]))\n",
    "    \n",
    "    # Avoid division by zero in case of zero magnitude\n",
    "    if magnitude_dict1 == 0 or magnitude_dict2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate cosine similarity (dot product divided by the product of the magnitudes)\n",
    "    cosine_similarity = dot_product / (magnitude_dict1 * magnitude_dict2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x17i627INSk"
   },
   "source": [
    "iii) Preprocess the string `query` and convert it to a tfidf representation. Then calculate its cosine similarity to all the documents in the dictionary `tfidf`.\n",
    "\n",
    "For any document with a non-zero similarity, print out the similarity, source and language of the original quotation.\n",
    "\n",
    "(8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2Mf_pHqq4kzk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.3922, Source: John Kennedy, Language: English\n",
      "Similarity: 0.3592, Source: the Bible, Language: Greek\n",
      "Similarity: 0.2020, Source: the Bible, Language: Greek\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the query\n",
    "# Tokenize\n",
    "query_tokens = word_tokenize(query)\n",
    "# Normalize: convert to lowercase and remove stopwords and punctuation\n",
    "query_normalised = [token.lower() for token in query_tokens if token.lower() not in stop and token not in punctuations]\n",
    "\n",
    "# Calculate term frequency for the query\n",
    "query_tf = {}\n",
    "for token in query_normalised:\n",
    "    query_tf[token] = query_tf.get(token, 0) + 1/len(query_normalised)\n",
    "\n",
    "# Calculate tfidf for the query\n",
    "query_tfidf = {}\n",
    "for token, tf_value in query_tf.items():\n",
    "    # Use the idf value from the idf dictionary, if the token is not found, assume 0 idf (log2(N/1) = 0)\n",
    "    query_tfidf[token] = tf_value * idf.get(token, log2(N))\n",
    "\n",
    "# Calculate cosine similarity between the query tfidf and each document tfidf\n",
    "for key, doc_tfidf in tfidf.items():\n",
    "    similarity = sim(query_tfidf, doc_tfidf)\n",
    "    # If the similarity is non-zero, print out the similarity, source, and language\n",
    "    if similarity > 0:\n",
    "        print(f'Similarity: {similarity:.4f}, Source: {sources[key]}, Language: {languages[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
