{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "259FEqEb5xRr"
   },
   "source": [
    "#Question 2 (50 Marks)\n",
    "\n",
    "This question is about POS-tagging and Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89Wx4WPa5weU",
    "outputId": "c2420d26-1158-45e2-a245-7b4b280bb5f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/ferhatsarikaya/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ferhatsarikaya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# This is a list of sentences written in various styles.\n",
    "sentences=[\"a tediously verbose sentence may contain many gratuitous and overly contrived modifiers .\",\n",
    "           \"another sentence could be too short .\",\n",
    "           \"some people write sentences that contain nouns and verbs , avoiding adjectives and descriptions .\"]\n",
    "\n",
    "# This is a dictionary containing counts of pos tags from a corpus of sentences which were labelled as styles A and B.\n",
    "classtagcounts={\"A\":{\"RB\":30, \"JJ\":30, \"NN\":10, \"NNS\":10, \"VB\":10, \"VBD\":10},\n",
    "                \"B\":{\"VBP\":20, \"VBZ\":10, \"VBG\":10, \"VBD\":10, \"NN\":20, \"NNS\":30}}\n",
    "\n",
    "# This is a complete list of pos tags.\n",
    "taglist = list(nltk.data.load('help/tagsets/upenn_tagset.pickle').keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI7AfgwoH1-q"
   },
   "source": [
    "a) By following the steps below, pos-tag the sentences and construct a bag-of-tags representation of each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QcL3UAsIl2m"
   },
   "source": [
    "i) Use the method `.split()` and the function `pos_tag()`, which has been imported above, to turn the list `sentences` into a list, named `tagged`, of lists of tuples containing word,tag pairs.\n",
    "\n",
    "So, for example, the list `[\"this is a sentence\"]` would become `[[('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN')]]`.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jaXDI-PKHyiP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences:\n",
      "[[('a', 'DT'), ('tediously', 'RB'), ('verbose', 'JJ'), ('sentence', 'NN'), ('may', 'MD'), ('contain', 'VB'), ('many', 'JJ'), ('gratuitous', 'JJ'), ('and', 'CC'), ('overly', 'RB'), ('contrived', 'VBD'), ('modifiers', 'NNS'), ('.', '.')], [('another', 'DT'), ('sentence', 'NN'), ('could', 'MD'), ('be', 'VB'), ('too', 'RB'), ('short', 'JJ'), ('.', '.')], [('some', 'DT'), ('people', 'NNS'), ('write', 'VBP'), ('sentences', 'NNS'), ('that', 'WDT'), ('contain', 'VBP'), ('nouns', 'NNS'), ('and', 'CC'), ('verbs', 'NNS'), (',', ','), ('avoiding', 'VBG'), ('adjectives', 'NNS'), ('and', 'CC'), ('descriptions', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Split and POS-tag the sentences\n",
    "tagged = [pos_tag(sentence.split()) for sentence in sentences]\n",
    "\n",
    "# Print the tagged sentences to verify the output\n",
    "print(\"Tagged sentences:\")\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I58Gw_eqKdht"
   },
   "source": [
    "ii) Convert each list of word,tag pairs into a bag-of-tags representation using the FreqDist class, which has been imported above.\n",
    "\n",
    "So, for example `[[('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN')]]` would become `[FreqDist({'DT': 2, 'VBZ': 1, 'NN': 1})]`.\n",
    "\n",
    "(7 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VQPEDcxn5kWR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of tags:\n",
      "Sentence 1: <FreqDist with 10 samples and 13 outcomes>\n",
      "Sentence 2: <FreqDist with 7 samples and 7 outcomes>\n",
      "Sentence 3: <FreqDist with 8 samples and 15 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Convert each list of word,tag pairs into a bag-of-tags representation\n",
    "bag_of_tags = [FreqDist(tags for word, tags in sentence) for sentence in tagged]\n",
    "\n",
    "# Print the bag of tags to verify the output\n",
    "print(\"Bag of tags:\")\n",
    "for index, freq_dist in enumerate(bag_of_tags):\n",
    "    print(f\"Sentence {index+1}: {freq_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMK72_eoMdLD"
   },
   "source": [
    "b) By following the steps below, calculate the parameters of a Naive Bayes model and predict the probability of each sentence being a member of classes A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5hl7NXnKb26"
   },
   "source": [
    "i) Explain the ideas behind the Naive Bayes model.\n",
    "\n",
    "Starting from the assumption that we want to find the class that maximises $p(class|document)$, explain how Bayes theorem is used and what naive assumption is made about the features in the document. Describe the priors and conditional probabilities that are used to predict the most likely class for a document.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OMNFQ5dLIGJ"
   },
   "source": [
    "The Naive Bayes model is a probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features. It is particularly suited for classification tasks where the dimensionality of the input is high, such as text classification.\n",
    "\n",
    "<b>Bayes Theorem</b>: Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For a classification task, it can be written as:\n",
    "\n",
    "$[ P(class|document) = \\frac{P(document|class) \\times P(class)}{P(document)} ]$\n",
    "\n",
    "In the context of Naive Bayes:\n",
    "<ul>\n",
    "    <li>( $P(class|document)$ ) is the posterior probability of a class given a document.</li>\n",
    "    <li>( $P(document|class)$ ) is the likelihood which is the probability of a document given a class.</li>\n",
    "    <li>( $P(class)$ ) is the class prior probability, and it is the probability of a class occurring in the dataset.</li>\n",
    "    <li>( $P(document)$ ) is the evidence, the probability of a document occurring.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Naive Assumption</b>: The 'naive' assumption made in the Naive Bayes classifier is that each feature (in this case, each word or tag in a document) is independent of every other feature given the class. This means that the presence or absence of a particular feature is unrelated to the presence or absence of any other feature, conditional on the class. This assumption is generally not true in real-world settings (words often appear in context with other words), but the classifier often performs well despite this simplification.\n",
    "\n",
    "<b>Priors and Conditional Probabilities</b>: In the Naive Bayes model, the priors represent our initial beliefs about the probability of each class before observing any data. The conditional probabilities, also known as likelihoods, represent the probability of observing a given feature in documents belonging to a particular class.\n",
    "\n",
    "To predict the most likely class for a document, the Naive Bayes classifier calculates the posterior probability for each class and selects the class with the highest posterior probability. The process involves:\n",
    "<ol>\n",
    "    <li>Calculating the prior probability for each class.</li>\n",
    "    <li>Calculating the conditional probability of each feature given a class using the frequency of the feature in documents of that class.</li>\n",
    "    <li>Multiplying the prior probability by the conditional probabilities of all features found in the document.</li>\n",
    "    <li>Dividing by the evidence, which can often be ignored in practice since it's the same for all classes and we're interested in the most likely class, not the absolute probability.</li>\n",
    "</ol>\n",
    "\n",
    "The class with the highest posterior probability is then predicted as the most likely class to which the document belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zugBLa8oMyeZ"
   },
   "source": [
    "ii) Sum the counts in `classtagcounts` to get total frequencies for classes `A` and `B` and put the results in a dictionary called `classcounts`.\n",
    "\n",
    "Then create a dictionary, called `classpriors`, containing the prior probabilities for each class based on the frequencies in the dictionary `classcounts`.\n",
    "\n",
    "(7 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s0jxsQBTyRq8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: {'A': 100, 'B': 100}\n",
      "Class Priors: {'A': 0.5, 'B': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Sum the counts in classtagcounts to get total frequencies for classes A and B\n",
    "classcounts = {class_name: sum(tag_counts.values()) for class_name, tag_counts in classtagcounts.items()}\n",
    "\n",
    "# Calculate the total number of instances across all classes\n",
    "total_instances = sum(classcounts.values())\n",
    "\n",
    "# Create a dictionary containing the prior probabilities for each class\n",
    "classpriors = {class_name: class_count / total_instances for class_name, class_count in classcounts.items()}\n",
    "\n",
    "# Print the results to verify the output\n",
    "print(\"Class Counts:\", classcounts)\n",
    "print(\"Class Priors:\", classpriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLMaRpK6NcyV"
   },
   "source": [
    "iii) Define a function, `condprobs()`, which will take two inputs - a dictionary mapping class labels to dictionaries of feature frequencies and a dictionary mapping class labels to frequencies - and produce a dictionary of dictionaries containing conditional probabilities. Each key in the outer dictionary should be a class label, the inner keys should be features and the inner values should be conditional probabilities: $$p(feature|label)$$.\n",
    "\n",
    "Apply this function to the `classtagcounts` and `classcounts` dictionaries defined above.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gfiF8zqVyv4z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Probabilities:\n",
      "A: {'RB': 0.3, 'JJ': 0.3, 'NN': 0.1, 'NNS': 0.1, 'VB': 0.1, 'VBD': 0.1}\n",
      "B: {'VBP': 0.2, 'VBZ': 0.1, 'VBG': 0.1, 'VBD': 0.1, 'NN': 0.2, 'NNS': 0.3}\n"
     ]
    }
   ],
   "source": [
    "def condprobs(feature_counts, class_freqs):\n",
    "    # Initialize the dictionary for conditional probabilities\n",
    "    cond_probs = {}\n",
    "    # Iterate over each class label\n",
    "    for class_label, features in feature_counts.items():\n",
    "        # Calculate the sum of frequencies of all features for the class\n",
    "        total_freq = class_freqs[class_label]\n",
    "        # Initialize the dictionary for the current class\n",
    "        cond_probs[class_label] = {}\n",
    "        # Iterate over each feature in the class\n",
    "        for feature, count in features.items():\n",
    "            # Calculate the conditional probability and assign it to the feature\n",
    "            cond_probs[class_label][feature] = count / total_freq\n",
    "    return cond_probs\n",
    "\n",
    "# Apply the function to classtagcounts and classcounts\n",
    "conditional_probabilities = condprobs(classtagcounts, classcounts)\n",
    "\n",
    "# Print the conditional probabilities to verify the output\n",
    "print(\"Conditional Probabilities:\")\n",
    "for class_label, probs in conditional_probabilities.items():\n",
    "    print(f\"{class_label}: {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wqoSZo_Miju"
   },
   "source": [
    "iv) Explain why we might want to smooth the conditional probabilities, and how add one smoothing works.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNaxUzqoMzDI"
   },
   "source": [
    "<b>Reason for Smoothing Conditional Probabilities</b>:\n",
    "Smoothing is applied to conditional probabilities in Naive Bayes to handle the problem of zero probability. In a dataset, it is possible that certain features (in this context, POS tags) may not appear in the training data for a particular class. Without smoothing, the conditional probability of that feature given the class would be zero. Since Naive Bayes multiplies probabilities together, a single zero probability would result in a zero probability for the entire class, regardless of the other evidence. This is clearly not desirable as it can lead to poor model performance, particularly on unseen data.\n",
    "\n",
    "<b>How Add-One (Laplace) Smoothing Works</b>:\n",
    "Add-one smoothing, also known as Laplace smoothing, is a technique used to smooth categorical data. The idea is simple: add one to the count of every feature for every class. This corresponds to pretending that we've seen each feature once more than we actually have, to avoid any zero counts.\n",
    "\n",
    "In mathematical terms, for a given class ($c$) and feature ($f$), the smoothed conditional probability is calculated as:\n",
    "\n",
    "$[ P(f|c) = \\frac{count(f, c) + 1}{\\sum_{f' \\in F} (count(f', c) + 1)} ]$\n",
    "\n",
    "where:\n",
    "<ul>\n",
    "    <li>$( count(f, c) )$ is the original count of feature ($f$) in class ($c$).</li>\n",
    "    <li>( $F$ ) is the set of all features.</li>\n",
    "    <li>( $sum_{f' \\in F} (count(f', c) + 1)$ ) is the sum of the counts of all features in class ($c$) after adding one to each count; this also equals the total count of all features in class ($c$) plus the number of features ( $|F|$ ).</li>\n",
    "</ul>\n",
    "\n",
    "The denominator normalizes the probability so that the sum of probabilities of all features for a given class equals 1.\n",
    "\n",
    "This method has the effect of shifting probability mass from frequent features to infrequent ones, thereby reducing the impact of having zero counts for certain features in the training data. It also helps the model generalize better to unseen data by not being overly confident about the non-occurrence of a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWC75MDybwxA"
   },
   "source": [
    "v) Using the list of possible tags, `taglist`, apply add one smoothing to the `classtagcounts` dictionary and update the `classcounts` dictionary to reflect the modified class frequencies.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XO0ZgXRNzPHA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed Class Counts: {'A': 145, 'B': 145}\n",
      "Smoothed Class Tag Counts:\n",
      "A: {'LS': 1, 'TO': 1, 'VBN': 1, \"''\": 1, 'WP': 1, 'UH': 1, 'VBG': 1, 'JJ': 31, 'VBZ': 1, '--': 1, 'VBP': 1, 'NN': 11, 'DT': 1, 'PRP': 1, ':': 1, 'WP$': 1, 'NNPS': 1, 'PRP$': 1, 'WDT': 1, '(': 1, ')': 1, '.': 1, ',': 1, '``': 1, '$': 1, 'RB': 31, 'RBR': 1, 'RBS': 1, 'VBD': 11, 'IN': 1, 'FW': 1, 'RP': 1, 'JJR': 1, 'JJS': 1, 'PDT': 1, 'MD': 1, 'VB': 11, 'WRB': 1, 'NNP': 1, 'EX': 1, 'NNS': 11, 'SYM': 1, 'CC': 1, 'CD': 1, 'POS': 1}\n",
      "B: {'LS': 1, 'TO': 1, 'VBN': 1, \"''\": 1, 'WP': 1, 'UH': 1, 'VBG': 11, 'JJ': 1, 'VBZ': 11, '--': 1, 'VBP': 21, 'NN': 21, 'DT': 1, 'PRP': 1, ':': 1, 'WP$': 1, 'NNPS': 1, 'PRP$': 1, 'WDT': 1, '(': 1, ')': 1, '.': 1, ',': 1, '``': 1, '$': 1, 'RB': 1, 'RBR': 1, 'RBS': 1, 'VBD': 11, 'IN': 1, 'FW': 1, 'RP': 1, 'JJR': 1, 'JJS': 1, 'PDT': 1, 'MD': 1, 'VB': 1, 'WRB': 1, 'NNP': 1, 'EX': 1, 'NNS': 31, 'SYM': 1, 'CC': 1, 'CD': 1, 'POS': 1}\n"
     ]
    }
   ],
   "source": [
    "# Apply add-one smoothing to classtagcounts and update classcounts\n",
    "smoothed_classtagcounts = {}\n",
    "smoothed_classcounts = {}\n",
    "\n",
    "# Iterate over each class label\n",
    "for class_label, features in classtagcounts.items():\n",
    "    # Apply smoothing to the feature counts\n",
    "    smoothed_classtagcounts[class_label] = {feature: count + 1 for feature in taglist for count in [features.get(feature, 0)]}\n",
    "    # Update the class frequency to include the additional counts from smoothing\n",
    "    # We add len(taglist) because we're adding one count for each tag\n",
    "    smoothed_classcounts[class_label] = classcounts[class_label] + len(taglist)\n",
    "\n",
    "# Print the smoothed class frequencies and tag counts to verify the output\n",
    "print(\"Smoothed Class Counts:\", smoothed_classcounts)\n",
    "print(\"Smoothed Class Tag Counts:\")\n",
    "for class_label, features in smoothed_classtagcounts.items():\n",
    "    print(f\"{class_label}: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwY1tY1wccVh"
   },
   "source": [
    "vi) Call the `condprobs` function on the smoothed frequencies.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iJ6Gbwgqm6sm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed Conditional Probabilities:\n",
      "A: {'LS': 0.006896551724137931, 'TO': 0.006896551724137931, 'VBN': 0.006896551724137931, \"''\": 0.006896551724137931, 'WP': 0.006896551724137931, 'UH': 0.006896551724137931, 'VBG': 0.006896551724137931, 'JJ': 0.21379310344827587, 'VBZ': 0.006896551724137931, '--': 0.006896551724137931, 'VBP': 0.006896551724137931, 'NN': 0.07586206896551724, 'DT': 0.006896551724137931, 'PRP': 0.006896551724137931, ':': 0.006896551724137931, 'WP$': 0.006896551724137931, 'NNPS': 0.006896551724137931, 'PRP$': 0.006896551724137931, 'WDT': 0.006896551724137931, '(': 0.006896551724137931, ')': 0.006896551724137931, '.': 0.006896551724137931, ',': 0.006896551724137931, '``': 0.006896551724137931, '$': 0.006896551724137931, 'RB': 0.21379310344827587, 'RBR': 0.006896551724137931, 'RBS': 0.006896551724137931, 'VBD': 0.07586206896551724, 'IN': 0.006896551724137931, 'FW': 0.006896551724137931, 'RP': 0.006896551724137931, 'JJR': 0.006896551724137931, 'JJS': 0.006896551724137931, 'PDT': 0.006896551724137931, 'MD': 0.006896551724137931, 'VB': 0.07586206896551724, 'WRB': 0.006896551724137931, 'NNP': 0.006896551724137931, 'EX': 0.006896551724137931, 'NNS': 0.07586206896551724, 'SYM': 0.006896551724137931, 'CC': 0.006896551724137931, 'CD': 0.006896551724137931, 'POS': 0.006896551724137931}\n",
      "B: {'LS': 0.006896551724137931, 'TO': 0.006896551724137931, 'VBN': 0.006896551724137931, \"''\": 0.006896551724137931, 'WP': 0.006896551724137931, 'UH': 0.006896551724137931, 'VBG': 0.07586206896551724, 'JJ': 0.006896551724137931, 'VBZ': 0.07586206896551724, '--': 0.006896551724137931, 'VBP': 0.14482758620689656, 'NN': 0.14482758620689656, 'DT': 0.006896551724137931, 'PRP': 0.006896551724137931, ':': 0.006896551724137931, 'WP$': 0.006896551724137931, 'NNPS': 0.006896551724137931, 'PRP$': 0.006896551724137931, 'WDT': 0.006896551724137931, '(': 0.006896551724137931, ')': 0.006896551724137931, '.': 0.006896551724137931, ',': 0.006896551724137931, '``': 0.006896551724137931, '$': 0.006896551724137931, 'RB': 0.006896551724137931, 'RBR': 0.006896551724137931, 'RBS': 0.006896551724137931, 'VBD': 0.07586206896551724, 'IN': 0.006896551724137931, 'FW': 0.006896551724137931, 'RP': 0.006896551724137931, 'JJR': 0.006896551724137931, 'JJS': 0.006896551724137931, 'PDT': 0.006896551724137931, 'MD': 0.006896551724137931, 'VB': 0.006896551724137931, 'WRB': 0.006896551724137931, 'NNP': 0.006896551724137931, 'EX': 0.006896551724137931, 'NNS': 0.21379310344827587, 'SYM': 0.006896551724137931, 'CC': 0.006896551724137931, 'CD': 0.006896551724137931, 'POS': 0.006896551724137931}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the conditional probabilities using the smoothed frequencies\n",
    "smoothed_conditional_probabilities = condprobs(smoothed_classtagcounts, smoothed_classcounts)\n",
    "\n",
    "# Print the smoothed conditional probabilities to verify the output\n",
    "print(\"Smoothed Conditional Probabilities:\")\n",
    "for class_label, probs in smoothed_conditional_probabilities.items():\n",
    "    print(f\"{class_label}: {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsA7z6NOcmHE"
   },
   "source": [
    "vii) Use the class priors and conditional probabilities you have just calculated to derive probabilities of belonging to the classes A and B for each sentence in `sentences`. For each sentence, predict the most likely class and print out each original sentence alongside your prediction.\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "y-g--614nQTH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'a tediously verbose sentence may contain many gratuitous and overly contrived modifiers .'\n",
      "Predicted Class: A\n",
      "\n",
      "Sentence: 'another sentence could be too short .'\n",
      "Predicted Class: A\n",
      "\n",
      "Sentence: 'some people write sentences that contain nouns and verbs , avoiding adjectives and descriptions .'\n",
      "Predicted Class: B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "# Function to predict the most likely class for each sentence\n",
    "def predict_class(sentences, class_priors, conditional_probs, taglist):\n",
    "    predictions = []\n",
    "    for sentence in sentences:\n",
    "        # Extract the bag-of-tags for the sentence\n",
    "        sentence_tags = pos_tag(sentence.split())\n",
    "        bag_of_tags = FreqDist(tags for word, tags in sentence_tags)\n",
    "\n",
    "        # Initialize the log probability for each class\n",
    "        sentence_log_probs = {class_label: log(class_priors[class_label]) for class_label in class_priors.keys()}\n",
    "\n",
    "        # Calculate the log probability for each class\n",
    "        for class_label, class_log_prior in sentence_log_probs.items():\n",
    "            for tag, freq in bag_of_tags.items():\n",
    "                # Check if the tag is in our conditional probabilities dictionary\n",
    "                if tag in conditional_probs[class_label]:\n",
    "                    # Add the log probabilities (to avoid underflow)\n",
    "                    sentence_log_probs[class_label] += freq * log(conditional_probs[class_label][tag])\n",
    "\n",
    "        # Predict the class with the highest log probability\n",
    "        predicted_class = max(sentence_log_probs, key=sentence_log_probs.get)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "        # Print the original sentence alongside the prediction\n",
    "        print(f\"Sentence: '{sentence}'\")\n",
    "        print(f\"Predicted Class: {predicted_class}\\n\")\n",
    "    return predictions\n",
    "\n",
    "# Call the prediction function\n",
    "predictions = predict_class(sentences, classpriors, smoothed_conditional_probabilities, taglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
